{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('senate_data.csv', parse_dates=['Transaction Date'])\n",
    "#only working on Stocks\n",
    "df = df[df['Type'] == 'Stock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop exchanges\n",
    "df = df[df['Buy/Sell'] != 'Exchange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just going to drop the irregular rows\n",
    "df = df[df['Stock'].apply(lambda x: len(x) < 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaN in type to stock\n",
    "df['Type'].fillna('Stock', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stock data for each day, drop non found (also only US)\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "#replace FB with META\n",
    "df['Stock'] = df['Stock'].replace('FB', 'META')\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    stock = row['Stock']\n",
    "    transaction_date = row['Transaction Date'].date()\n",
    "    ticker = yf.Ticker(stock)\n",
    "    data = ticker.history(start=transaction_date, end=transaction_date+datetime.timedelta(days=1))\n",
    "    if data.empty:\n",
    "        df.drop(i, inplace=True) \n",
    "    else:\n",
    "        df.loc[i, 'Price'] = data['Close'].values[0]\n",
    "\n",
    "#resave as cleaned csf and save prices as json to be used for later analysis\n",
    "df.to_csv('senate_data_clean.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#irregular stocks to fix\n",
    "irregular = df[df['Stock'].apply(lambda x: len(x) > 5)]\n",
    "#drop all rows that are weird in the stock column;\n",
    "irregular = irregular[~irregular['Stock'].str.contains('bond|bds|due|%', case=False)]\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "#try to search up ticker\n",
    "driver = webdriver.Firefox()\n",
    "names = {}\n",
    "#workaround for startup web scraping block - going slowly\n",
    "count = 0\n",
    "sleep = 0\n",
    "for stock in irregular['Stock']:\n",
    "    count += 1\n",
    "    sleep += 1\n",
    "    driver.get('https://www.startpage.com')\n",
    "    search_bar = driver.find_element(By.ID, 'q') \n",
    "    og = stock\n",
    "    if '\\n' in stock:\n",
    "        stock = stock.split('\\n')[0]\n",
    "    search_bar.send_keys(f\"{stock} stock symbol yahoo finance\")\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    time.sleep(6) \n",
    "    try: #yahoo\n",
    "        search_results = driver.find_element(By.XPATH, \"//a[contains(@href, 'https://finance.yahoo.com/quote/')]\")\n",
    "        search_results = search_results.text.split('/')[-2]\n",
    "        names[og] = search_results\n",
    "    except:\n",
    "        names[og] = 'N/A'\n",
    "    time.sleep(6)\n",
    "    if count == 20:\n",
    "        count = 0\n",
    "        time.sleep(600)\n",
    "    if sleep == 200:\n",
    "        sleep = 0\n",
    "        time.sleep(6000)\n",
    "driver.quit()\n",
    "with open('clean_irregulars.json', 'w') as f:\n",
    "    json.dump(names, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
